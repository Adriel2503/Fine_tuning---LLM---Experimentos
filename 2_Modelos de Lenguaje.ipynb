{"cells":[{"cell_type":"markdown","metadata":{"id":"XN4aCqNKD5Ol"},"source":["# Instalar dependencias"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19670,"status":"ok","timestamp":1720882300354,"user":{"displayName":"Ronald Cardenas","userId":"11716680880798197691"},"user_tz":-60},"id":"4s4zZ3vGRvhq","outputId":"d31b8627-0ad6-443b-ed7e-823295f55427"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"rRCXhhcCnOv8"},"source":[]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":234,"status":"ok","timestamp":1720882303458,"user":{"displayName":"Ronald Cardenas","userId":"11716680880798197691"},"user_tz":-60},"id":"aPfHWkSMR4o9"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":296,"status":"ok","timestamp":1720882305315,"user":{"displayName":"Ronald Cardenas","userId":"11716680880798197691"},"user_tz":-60},"id":"PVzSViWcvg2U"},"outputs":[],"source":["%load_ext tensorboard\n","import datetime, os"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":233,"status":"ok","timestamp":1720882310611,"user":{"displayName":"Ronald Cardenas","userId":"11716680880798197691"},"user_tz":-60},"id":"ntUvqg68SRe7","outputId":"e353f355-a488-4d5a-d9f9-64ef23328706"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/nlp-applications-class/semana2\n"]}],"source":["%cd  /content/drive/MyDrive/nlp-applications-class/semana2/"]},{"cell_type":"code","execution_count":5,"metadata":{"collapsed":true,"id":"MZ6-oP8ZA6ol","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720882408182,"user_tz":-60,"elapsed":95726,"user":{"displayName":"Ronald Cardenas","userId":"11716680880798197691"}},"outputId":"ef5f140c-ecc5-4c1d-b3ee-d42280114b9e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting accelerate>=0.12.0 (from -r requirements.txt (line 1))\n","  Downloading accelerate-0.32.1-py3-none-any.whl (314 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting datasets>=1.8.0 (from -r requirements.txt (line 2))\n","  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (0.1.99)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (1.11.4)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.2.2)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (3.20.3)\n","Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (2.3.0+cu121)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (4.41.2)\n","Collecting evaluate (from -r requirements.txt (line 9))\n","  Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (24.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (6.0.1)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (0.23.4)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (0.4.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (3.15.4)\n","Collecting pyarrow>=15.0.0 (from datasets>=1.8.0->-r requirements.txt (line 2))\n","  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (0.6)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets>=1.8.0->-r requirements.txt (line 2))\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (2.0.3)\n","Collecting requests>=2.32.2 (from datasets>=1.8.0->-r requirements.txt (line 2))\n","  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (4.66.4)\n","Collecting xxhash (from datasets>=1.8.0->-r requirements.txt (line 2))\n","  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets>=1.8.0->-r requirements.txt (line 2))\n","  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (3.9.5)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (3.5.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 7)) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 7)) (1.13.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 7)) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 7)) (3.1.4)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.3->-r requirements.txt (line 7))\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.3->-r requirements.txt (line 7))\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.3->-r requirements.txt (line 7))\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.3->-r requirements.txt (line 7))\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.3->-r requirements.txt (line 7))\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.3->-r requirements.txt (line 7))\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.3->-r requirements.txt (line 7))\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.3->-r requirements.txt (line 7))\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.3->-r requirements.txt (line 7))\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.3->-r requirements.txt (line 7))\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.3->-r requirements.txt (line 7))\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 7)) (2.3.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.3->-r requirements.txt (line 7))\n","  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 8)) (2024.5.15)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 8)) (0.19.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (4.0.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=1.8.0->-r requirements.txt (line 2)) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=1.8.0->-r requirements.txt (line 2)) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=1.8.0->-r requirements.txt (line 2)) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=1.8.0->-r requirements.txt (line 2)) (2024.7.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3->-r requirements.txt (line 7)) (2.1.5)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (2024.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3->-r requirements.txt (line 7)) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (1.16.0)\n","Installing collected packages: xxhash, requests, pyarrow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets, evaluate, accelerate\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.31.0\n","    Uninstalling requests-2.31.0:\n","      Successfully uninstalled requests-2.31.0\n","  Attempting uninstall: pyarrow\n","    Found existing installation: pyarrow 14.0.2\n","    Uninstalling pyarrow-14.0.2:\n","      Successfully uninstalled pyarrow-14.0.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n","google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n","ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed accelerate-0.32.1 datasets-2.20.0 dill-0.3.8 evaluate-0.4.2 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 pyarrow-16.1.0 requests-2.32.3 xxhash-3.4.1\n"]}],"source":["!pip install -r requirements.txt"]},{"cell_type":"markdown","metadata":{"id":"Zy64pKggJiG9"},"source":["# Usando modelos pre-entrenados de HuggingFace\n","En esta section vamos a probar modelos disponibles en [HuggingFace](https://https://huggingface.co/), un repositorio masivo de modelos y datasets para diversas aplicaciones de NLP, Vision, Audio, entre otros.\n","\n","Como libreria backbone emplearemos Transformers, que a su vez extiende pytorch."]},{"cell_type":"markdown","metadata":{"id":"qSuGiSTWx3l8"},"source":["## Pipeline para usar un modelo por defecto\n","GPT2 - 124M parametros"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":1824,"status":"ok","timestamp":1720883465982,"user":{"displayName":"Ronald Cardenas","userId":"11716680880798197691"},"user_tz":-60},"id":"kTm3F2TGDNW5"},"outputs":[],"source":["from transformers import pipeline, set_seed\n","\n","generator = pipeline('text-generation', model='gpt2')\n","\n","set_seed(42)"]},{"cell_type":"markdown","source":["Greedy Search (Busqueda Avara)"],"metadata":{"id":"cAx1Vv59Jaxh"}},{"cell_type":"code","source":["generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=1, do_sample=False)"],"metadata":{"id":"wv5cABAMJmDC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720855277500,"user_tz":-60,"elapsed":3000,"user":{"displayName":"Ronald Cardenas","userId":"11716680880798197691"}},"outputId":"b8b88ad7-d7db-4e1b-e70b-d42417f866ca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"execute_result","data":{"text/plain":["[{'generated_text': \"Hello, I'm a language model, not a programming language. I'm a language model. I'm a language model. I'm a language model\"}]"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["Beam Search"],"metadata":{"id":"GFVJ8XohJ3Mn"}},{"cell_type":"code","source":["generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)"],"metadata":{"id":"ZQKiV-fvJ6pY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720855300672,"user_tz":-60,"elapsed":9951,"user":{"displayName":"Ronald Cardenas","userId":"11716680880798197691"}},"outputId":"42aad3c7-9100-4549-e701-80064cd90b35"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"execute_result","data":{"text/plain":["[{'generated_text': \"Hello, I'm a language model, but what I'm really doing is making a human-readable document. There are other languages, but those are\"},\n"," {'generated_text': \"Hello, I'm a language model, not a syntax model. That's why I like it. I've done a lot of programming projects.\\n\"},\n"," {'generated_text': \"Hello, I'm a language model, and I'll do it in no time!\\n\\nOne of the things we learned from talking to my friend\"},\n"," {'generated_text': \"Hello, I'm a language model, not a command line tool.\\n\\nIf my code is simple enough:\\n\\nif (use (string\"},\n"," {'generated_text': \"Hello, I'm a language model, I've been using Language in all my work. Just a small example, let's see a simplified example.\"}]"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["## Sampling (Muestreo)"],"metadata":{"id":"sUL6_bq_J_L8"}},{"cell_type":"markdown","source":["Ancestral Sampling (Muestreo de Ancestros)"],"metadata":{"id":"qocgegxHKIM0"}},{"cell_type":"code","source":["generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=1, do_sample=True, temperature=1.0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HNt0wbogKFPs","executionInfo":{"status":"ok","timestamp":1720858477695,"user_tz":-60,"elapsed":2549,"user":{"displayName":"Ronald Cardenas","userId":"11716680880798197691"}},"outputId":"5f1a2c2e-c76b-4520-cdde-d4c2eb1b9ba3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"execute_result","data":{"text/plain":["[{'generated_text': \"Hello, I'm a language model, but I'm not afraid of language models. Because the language model tells you, this isn't your normal,\"}]"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=1, do_sample=True, temperature=0.2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HFMDlI76IRmb","executionInfo":{"status":"ok","timestamp":1720884568586,"user_tz":-60,"elapsed":3019,"user":{"displayName":"Ronald Cardenas","userId":"11716680880798197691"}},"outputId":"1440d480-3cd5-44d2-94f7-21f1b8da4ce0"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"execute_result","data":{"text/plain":["[{'generated_text': \"Hello, I'm a language model, but I'm not a programmer. I'm a programmer. I'm a programmer. I'm a programmer.\"}]"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=1, do_sample=True, temperature=2.0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P1MggJZ4IZza","executionInfo":{"status":"ok","timestamp":1720884628120,"user_tz":-60,"elapsed":1730,"user":{"displayName":"Ronald Cardenas","userId":"11716680880798197691"}},"outputId":"b4de318a-88d3-4afd-e0b3-cb9f1220fbb7"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"execute_result","data":{"text/plain":["[{'generated_text': \"Hello, I'm a language model, there I must tell you: 'Hello!' means:\\n\\nthat my body understands this phrase by heart …\"}]"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["Top-K Sampling"],"metadata":{"id":"TUBjH_CGKeOq"}},{"cell_type":"code","source":["generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=1, do_sample=True, top_k=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rD9uYC_sKWjG","executionInfo":{"status":"ok","timestamp":1720741226826,"user_tz":-60,"elapsed":614,"user":{"displayName":"Ronald Cardenas","userId":"11716680880798197691"}},"outputId":"2a6e7881-2f80-41b2-9b05-30a1d8dbb53b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"execute_result","data":{"text/plain":["[{'generated_text': \"Hello, I'm a language model, not a programming language. I'm a language model. I'm a language model. I'm a language model\"}]"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=1, do_sample=True, top_k=10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B-z0S69kKj6T","executionInfo":{"status":"ok","timestamp":1720741237072,"user_tz":-60,"elapsed":578,"user":{"displayName":"Ronald Cardenas","userId":"11716680880798197691"}},"outputId":"f4f62d4a-e2f2-4b95-9db1-0af3e041997d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"execute_result","data":{"text/plain":["[{'generated_text': \"Hello, I'm a language model, and we use it in our language to represent the data. We want to be able to use that to generate\"}]"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=1, do_sample=True, top_k=1000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4UUWjDlmK5kU","executionInfo":{"status":"ok","timestamp":1720741245697,"user_tz":-60,"elapsed":1494,"user":{"displayName":"Ronald Cardenas","userId":"11716680880798197691"}},"outputId":"91280b4d-c225-44b9-b2ae-3c5701aeca4d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"execute_result","data":{"text/plain":["[{'generated_text': 'Hello, I\\'m a language model, not a digital model.\"\\n\\nTrouble facing those with small English skills, Greg Myers, a former'}]"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["Nucleous Sampling (Muestreo de Nucleo)"],"metadata":{"id":"CGPENDBiLNW5"}},{"cell_type":"code","source":["generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=1, do_sample=True, top_k=0, top_p=1.0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AD3xGLCqLMNY","executionInfo":{"status":"ok","timestamp":1720741887851,"user_tz":-60,"elapsed":644,"user":{"displayName":"Ronald Cardenas","userId":"11716680880798197691"}},"outputId":"6ef05a2f-5ff5-4fdd-a07a-5b88beb42573"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"execute_result","data":{"text/plain":["[{'generated_text': \"Hello, I'm a language model, I wrote this in C++, OCaml, Objective-C, C#... all the languages and\"}]"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=1, do_sample=True, top_k=0, top_p=0.8, temperature=0.7)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KTRpENt0LadS","executionInfo":{"status":"ok","timestamp":1720885381881,"user_tz":-60,"elapsed":2334,"user":{"displayName":"Ronald Cardenas","userId":"11716680880798197691"}},"outputId":"7f8373a5-7419-4e99-ec26-e7d271f0240d"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"execute_result","data":{"text/plain":["[{'generated_text': \"Hello, I'm a language model, not a programming model. It's just that you can't write programs in C without programming in C.\\n\"}]"]},"metadata":{},"execution_count":10}]}],"metadata":{"colab":{"provenance":[{"file_id":"1HN7tak2FNd_oMw9vpEtubFHur_aqlhYc","timestamp":1720738165154}],"authorship_tag":"ABX9TyNdEu2/RIAUbT/dYrLTXC27"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}